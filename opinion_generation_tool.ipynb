{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1f10220294/Synthetic-Opinion-Dataset-Framework/blob/main/opinion_generation_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XPr5ferEk85-",
        "outputId": "f38e4a37-ed4e-419b-fd13-73b6049c51d3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: ddgs in /usr/local/lib/python3.12/dist-packages (9.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: readability-lxml in /usr/local/lib/python3.12/dist-packages (0.8.4.1)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.75.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
            "Requirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.2)\n",
            "Requirement already satisfied: fake-useragent>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (2.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (from lxml[html_clean]->readability-lxml) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "MMR用のSentenceTransformerモデルをロード中...\n",
            "モデルのロード完了。\n",
            "Gradioインターフェース(V7 - 1件選定)を起動します...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://055587cc0dc6a91fff.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://055587cc0dc6a91fff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- ライブラリのインストール ---\n",
        "!pip install gradio openai pandas ddgs sentence-transformers numpy torch requests beautifulsoup4 readability-lxml anthropic\n",
        "!pip install --upgrade google-generativeai\n",
        "\n",
        "import gradio as gr\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import random\n",
        "import datetime\n",
        "import os\n",
        "import re # ★ 正規表現によるAI出力の解析\n",
        "from google.colab import userdata\n",
        "import time # ◀◀◀【変更点 1/5】レート制限（スリープ）のために追加\n",
        "from google.colab import drive # ◀◀◀【変更点 2/5】Google Drive連携のために追加\n",
        "\n",
        "# RAG / MMR\n",
        "from ddgs.ddgs import DDGS\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# スクレイピング\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document # readability をインポート\n",
        "\n",
        "# --- 定数定義 ---\n",
        "\n",
        "# ペルソナ属性\n",
        "PERSONA_ATTRIBUTES = {\n",
        "    \"gender\": [\"男性\", \"女性\", \"その他\"],\n",
        "    \"age\": [\"10代\", \"20代\", \"30代\", \"40代\", \"50代\", \"60代\", \"70代\", \"80代\", \"90代\"],\n",
        "    \"personality\": [\n",
        "        \"楽観的\", \"悲観的\", \"慎重\", \"感情的\", \"論理的\", \"直感的\",\n",
        "        \"現実的\", \"理想主義的\", \"内向的\", \"外向的\", \"協調的\", \"独立的\"\n",
        "    ],\n",
        "    \"tone\": [\n",
        "        \"丁寧\", \"断定的\", \"口語的\", \"攻撃的\", \"冷静\", \"情熱的\",\n",
        "        \"皮肉的\", \"分析的\", \"謙虚\", \"事務的\", \"友好的\"\n",
        "    ],\n",
        "    \"volume\": [\"50文字程度\", \"80文字程度\",\"100文字程度\",\"150文字程度\",'200文字程度']\n",
        "}\n",
        "\n",
        "# RAG / スクレイピング定数 (V6のまま)\n",
        "MAX_CONTEXT_CHARACTERS = 4000 #コンテキストの最大文字数\n",
        "CHUNK_SIZE = 600 #分割サイズ\n",
        "CHUNK_OVERLAP = 200 #分割時の重なり(分割時に区切れないように)\n",
        "SCRAPE_TOP_N = 5   #取得する上位のページ数\n",
        "SCRAPE_TIMEOUT = 5  #タイムアウト時間(読み込みが長すぎた場合)\n",
        "\n",
        "# MMRモデルのグローバルロード\n",
        "try:\n",
        "    print(\"MMR用のSentenceTransformerモデルをロード中...\")\n",
        "    mmr_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"モデルのロード完了。\")\n",
        "except Exception as e:\n",
        "    print(f\"SentenceTransformerモデルのロードに失敗しました: {e}\")\n",
        "    mmr_model = None\n",
        "\n",
        "# --- クライアント初期化ヘルパー ---\n",
        "\n",
        "def get_openai_client(api_key):\n",
        "    effective_api_key = api_key\n",
        "    if not effective_api_key:\n",
        "        print(\"UIにAPIキーが入力されていません。Colab Secretsから 'OPENAI_API_KEY' の読み込みを試みます...\")\n",
        "        try:\n",
        "            effective_api_key = userdata.get('OPENAI_API_KEY')\n",
        "        except Exception as e:\n",
        "            print(f\"Colab Secretsの読み込み失敗: {e}\")\n",
        "            pass\n",
        "    if not effective_api_key:\n",
        "         raise Exception(\"APIキーがUIにもColab Secretsにもありません。\")\n",
        "\n",
        "    client = openai.OpenAI(\n",
        "        api_key = effective_api_key,\n",
        "        # base_url = \"https://api.openai.iniad.org/api/v1/\",\n",
        "    )\n",
        "    return client\n",
        "\n",
        "def get_gemini_client(api_key, model_name):\n",
        "    \"\"\"\n",
        "    Geminiクライアントを初期化し、モデルオブジェクトを返す。\n",
        "    セーフティ設定を緩和し、悪意のある表現の生成を試みる。\n",
        "    \"\"\"\n",
        "    effective_api_key = api_key\n",
        "    if not effective_api_key:\n",
        "        print(\"UIにGoogle APIキーが入力されていません。Colab Secretsから 'GOOGLE_API_KEY' の読み込みを試みます...\")\n",
        "        try:\n",
        "            effective_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "        except Exception as e:\n",
        "            print(f\"Colab Secretsの読み込み失敗: {e}\")\n",
        "            pass\n",
        "    if not effective_api_key:\n",
        "         raise Exception(\"Google APIキーがUIにもColab Secretsにもありません。\")\n",
        "\n",
        "    genai.configure(api_key=effective_api_key)\n",
        "\n",
        "    # ★ 悪意のある表現（対人攻撃など）の生成を許可するため、セーフティ設定を緩和\n",
        "    # これがないと、GeminiはAPIエラー（BLOCK_REASON_SAFETY）を返す可能性が高い\n",
        "    safety_settings = [\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    ]\n",
        "\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name,\n",
        "        safety_settings=safety_settings\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_claude_client(api_key):\n",
        "    \"\"\"\n",
        "    Anthropic (Claude) クライアントを初期化する。\n",
        "    \"\"\"\n",
        "    effective_api_key = api_key\n",
        "    if not effective_api_key:\n",
        "        print(\"UIにAnthropic APIキーが入力されていません。Colab Secretsから 'ANTHROPIC_API_KEY' の読み込みを試みます...\")\n",
        "        try:\n",
        "            effective_api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "        except Exception as e:\n",
        "            print(f\"Colab Secretsの読み込み失敗: {e}\")\n",
        "            pass\n",
        "    if not effective_api_key:\n",
        "         raise Exception(\"Anthropic APIキーがUIにもColab Secretsにもありません。\")\n",
        "\n",
        "    client = anthropic.Anthropic(\n",
        "        api_key=effective_api_key,\n",
        "    )\n",
        "    return client\n",
        "\n",
        "# --- 機能関数 (CSV) ---\n",
        "# ◀◀◀【変更】CSVの保存先を「Colabローカル」と「Google Drive」の両方に変更\n",
        "def save_to_csv(data_df):\n",
        "    try:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"generated_opinions_final_v7_lowest_{timestamp}.csv\" # 共通のファイル名\n",
        "\n",
        "        # --- 1. Colabローカル環境への保存 ---\n",
        "        data_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
        "        print(f\"CSVファイル '{filename}' をColabローカルに作成しました。\")\n",
        "\n",
        "        # --- 2. Google Driveへの保存 ---\n",
        "        drive_path = \"/content/drive/MyDrive/\"\n",
        "        full_drive_path = os.path.join(drive_path, filename) # Drive用のフルパス\n",
        "\n",
        "        # Driveのフォルダが存在するか確認（なければ作る）\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "        data_df.to_csv(full_drive_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"CSVファイル '{full_drive_path}' をGoogle Driveに作成しました。\")\n",
        "\n",
        "        # --- 戻り値は「ローカルのファイルパス」を返す (Gradioの File 出力コンポーネント用) ---\n",
        "        return filename, f\"✅ 生成とGoogle Drive/Colabローカルへの保存に成功しました。`{filename}` を確認してください。\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CSVファイルの保存中にエラー: {e}\")\n",
        "        return None, f\"❌ CSVファイルの保存中にエラーが発生しました: {e}\"\n",
        "\n",
        "# --- RAG（Web検索）関数 (変更なし) ---\n",
        "def perform_web_search(query, max_results=20):\n",
        "    print(f\"  ... 「{query}」のWeb検索を実行中 (Max {max_results}件)...\")\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            results = ddgs.text(query, max_results=max_results, region='jp-jp')\n",
        "        if not results:\n",
        "            print(\"  ... 検索結果が0件でした。\")\n",
        "            return []\n",
        "        search_data = [\n",
        "            {\"title\": r.get(\"title\", \"タイトルなし\"),\n",
        "             \"snippet\": r.get(\"body\", \"本文なし\"),\n",
        "             \"url\": r.get(\"href\", \"URLなし\")}\n",
        "            for r in results\n",
        "        ]\n",
        "        print(f\"  ... {len(search_data)}件の検索結果（フォールバック候補）を取得しました。\")\n",
        "        return search_data\n",
        "    except Exception as e:\n",
        "        print(f\"  ... Web検索中にエラーが発生しました: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- 【V6】readability ＋ 文字化け対策 ＋ Chunk600 (変更なし) ---\n",
        "def scrape_and_chunk_pages(search_results, top_n=5, chunk_size=600, overlap=200):\n",
        "    print(f\"  ... 高品質スクレイピング（Top {top_n}件, readability使用, Chunk={chunk_size}）を開始します...\")\n",
        "    high_quality_chunks = []\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Referer': 'https://www.google.com/'\n",
        "    }\n",
        "\n",
        "    target_urls = [res['url'] for res in search_results[:top_n] if res.get('url')]\n",
        "    if not target_urls:\n",
        "        print(\"  ... スクレイピング対象のURLがありませんでした。\")\n",
        "        return []\n",
        "\n",
        "    for url in target_urls:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=SCRAPE_TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "            response.encoding = response.apparent_encoding\n",
        "\n",
        "            doc = Document(response.text)\n",
        "            article_title = doc.title()\n",
        "            article_html = doc.summary()\n",
        "\n",
        "            soup = BeautifulSoup(article_html, 'html.parser')\n",
        "            text = soup.get_text(separator='\\n', strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "\n",
        "            if not text or len(text) < chunk_size / 2:\n",
        "                print(f\"  ... [失敗] {url} からreadabilityで十分なテキストを抽出できません。\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  ... [成功] {url} (Title: {article_title}) からテキスト抽出。チャンキング中...\")\n",
        "            start = 0\n",
        "            while start < len(text):\n",
        "                end = start + chunk_size\n",
        "                chunk = text[start:end]\n",
        "                high_quality_chunks.append({\n",
        "                    \"title\": f\"チャンク (Title: {article_title})\",\n",
        "                    \"snippet\": chunk,\n",
        "                    \"url\": url\n",
        "                })\n",
        "                start += chunk_size - overlap\n",
        "                if start + overlap >= len(text):\n",
        "                    break\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  ... [失敗] {url} のダウンロードエラー: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ... [失敗] {url} の解析エラー (readability等): {e}\")\n",
        "\n",
        "    print(f\"  ... スクレイピング完了。合計 {len(high_quality_chunks)} 件の高品質チャンクを生成しました。\")\n",
        "    return high_quality_chunks\n",
        "\n",
        "# --- MMR（多様性リランキング）関数 (変更なし) ---\n",
        "def apply_mmr(query, search_results, top_k=5, lambda_val=0.5):\n",
        "    valid_results = [res for res in search_results if res.get('snippet') and res.get('snippet').strip()]\n",
        "    if not mmr_model or not valid_results:\n",
        "        print(\"  ... MMRモデルがロードされていないか、有効なスニペットが空です。MMRをスキップします。\")\n",
        "        return search_results[:top_k]\n",
        "    print(f\"  ... MMR（λ={lambda_val}）を適用中... {len(valid_results)}件 -> {top_k}件\")\n",
        "    try:\n",
        "        query_embedding = mmr_model.encode(query, convert_to_tensor=True)\n",
        "        doc_embeddings = mmr_model.encode([res[\"snippet\"] for res in valid_results], convert_to_tensor=True)\n",
        "        cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
        "        selected_doc_indices = []\n",
        "        while len(selected_doc_indices) < min(top_k, len(valid_results)):\n",
        "            remaining_doc_indices = list(set(range(len(valid_results))) - set(selected_doc_indices))\n",
        "            mmr_scores = []\n",
        "            for i in remaining_doc_indices:\n",
        "                relevance_score = cosine_scores[i]\n",
        "                if not selected_doc_indices:\n",
        "                    diversity_score = 0\n",
        "                else:\n",
        "                    selected_embeddings = doc_embeddings[selected_doc_indices]\n",
        "                    similarity_to_selected = util.cos_sim(doc_embeddings[i], selected_embeddings)\n",
        "                    diversity_score = torch.max(similarity_to_selected)\n",
        "                mmr_score = lambda_val * relevance_score - (1 - lambda_val) * diversity_score\n",
        "                mmr_scores.append((mmr_score, i))\n",
        "            if not mmr_scores:\n",
        "                break\n",
        "            best_mmr_score, best_index = max(mmr_scores, key=lambda x: x[0])\n",
        "            selected_doc_indices.append(best_index)\n",
        "        mmr_results = [valid_results[i] for i in selected_doc_indices]\n",
        "        print(\"  ... MMRによる再ランキング完了。\")\n",
        "        return mmr_results\n",
        "    except Exception as e:\n",
        "        print(f\"  ... MMRの計算中にエラーが発生しました: {e}。MMRをスキップします。\")\n",
        "        return valid_results[:top_k]\n",
        "\n",
        "# --- AI関数群 ---\n",
        "\n",
        "# --- 【API 1/3】プロフィール ＋ 1クエリ ＋ 理由 (V7) ---\n",
        "def generate_profile_and_query(persona_dict, theme, stance,\n",
        "                               api_key, google_api_key, claude_api_key, # ◀ 引数追加\n",
        "                               model):\n",
        "    print(f\"    ... 【API 1/3】プロフィールと検索クエリ(＋理由)を同時生成中 (Model: {model})...\")\n",
        "\n",
        "    # (user_prompt は変更なし)\n",
        "    user_prompt = f\"\"\"\n",
        "    あなたはプロのプロフィール作家兼リサーチャーです。\n",
        "    以下の指示に従い、3つの項目（プロフィール、検索クエリ、理由）をリアリティをもって出力してください。\n",
        "    # 1. 人物属性\n",
        "    - 性別: {persona_dict['gender']}\n",
        "    - 年齢: {persona_dict['age']}\n",
        "    - 性格: {persona_dict['personality']}\n",
        "    - 口調: {persona_dict['tone']}\n",
        "    # 2. タスク\n",
        "    以下の3つの項目を、指定された形式で生成してください。\n",
        "    ---\n",
        "    プロフィール:\n",
        "    属性に基づき、以下の3要素を考慮したプロフィールを100〜200文字程度で創作してください。\n",
        "    1.【基本情報】: 性別、年齢、社会的役割、家族構成など\n",
        "    2.【パーソナリティ】: その社会的な役割や口調を反映した、その人の価値観\n",
        "    3.【生活の舞台や社会的立場】:生活環境や周囲との関係性（家族関係や交友関係など）やどんな立ち位置か\n",
        "---\n",
        "    検索クエリ:\n",
        "  （ここに、上記であなたが創作した「プロフィール」を持つ人物が、「{theme}」について「{stance}」の立場で情報収集する際に検索しそうな**具体的な検索クエリ**を**1つだけ**生成してください）\n",
        "    ---\n",
        "    理由:\n",
        "  （ここに、なぜその「プロフィール」の人物が、その「検索クエリ」を入力するに至ったのか、その思考プロセスや動機を簡潔に説明してください）\n",
        "---\n",
        "\"\"\"\n",
        "    fallback_profile = f\"（フォールバック: {persona_dict['age']} {persona_dict['gender']}, {persona_dict['personality']}）\"\n",
        "    fallback_query = f\"{theme} {stance} 理由\"\n",
        "    fallback_reason = \"フォールバック\"\n",
        "\n",
        "    try:\n",
        "        full_response_text = \"\"\n",
        "\n",
        "        # ▼▼▼ モデル分岐 ▼▼▼\n",
        "        if model.startswith(\"gemini-\"):\n",
        "            client = get_gemini_client(google_api_key, model)\n",
        "            generation_config = genai.types.GenerationConfig(temperature=1.0)\n",
        "            response = client.generate_content(user_prompt, generation_config=generation_config)\n",
        "            full_response_text = response.text\n",
        "\n",
        "        elif model.startswith(\"gpt-\"):\n",
        "            client = get_openai_client(api_key)\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=1.0,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            full_response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        elif model.startswith(\"claude-\"): # ◀◀◀ Claude分岐を追加\n",
        "            client = get_claude_client(claude_api_key)\n",
        "            response = client.messages.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=1.0,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            full_response_text = response.content[0].text\n",
        "\n",
        "        else:\n",
        "            raise Exception(f\"未対応のモデルです: {model}\")\n",
        "        # ▲▲▲ モデル分岐 ▲▲▲\n",
        "\n",
        "        # ★★★ 修正: 正規表現ロジックの強化 ★★★\n",
        "        # 区切り線(---)があってもなくても、次の項目の見出しが出現したらそこで区切る\n",
        "\n",
        "        # 1. プロフィール抽出\n",
        "        profile_match = re.search(r\"プロフィール[:：\\s]*(.*?)(?=\\n.*?検索クエリ[:：]|\\n.*?---|---|$)\", full_response_text, re.DOTALL | re.IGNORECASE)\n",
        "        profile_text = profile_match.group(1).strip() if profile_match else fallback_profile\n",
        "\n",
        "        # 2. 検索クエリ抽出\n",
        "        query_match = re.search(r\"検索クエリ[:：\\s]*(.*?)(?=\\n.*?理由[:：]|\\n.*?---|---|$)\", full_response_text, re.DOTALL | re.IGNORECASE)\n",
        "        query_text = query_match.group(1).strip() if query_match else fallback_query\n",
        "\n",
        "        # 3. 理由抽出\n",
        "        reason_match = re.search(r\"理由[:：\\s]*(.*)\", full_response_text, re.DOTALL | re.IGNORECASE)\n",
        "        reason_text = reason_match.group(1).strip() if reason_match else fallback_reason\n",
        "\n",
        "        print(f\"    ... API 1/3 完了。\")\n",
        "        print(f\"    ... プロフィール: {profile_text[:50]}...\")\n",
        "        print(f\"    ... 検索クエリ: {query_text}\")\n",
        "\n",
        "        return {\n",
        "            \"profile\": profile_text,\n",
        "            \"query\": query_text,\n",
        "            \"reason\": reason_text\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ... API 1/3 (プロフィール生成) エラー: {e}\")\n",
        "        if \"BLOCK_REASON_SAFETY\" in str(e):\n",
        "             print(\"    ... ★★★ Geminiのセーフティ機能によりブロックされました ★★★\")\n",
        "             return {\"profile\": f\"（API 1/3 エラー: Geminiによりブロック）\", \"query\": fallback_query, \"reason\": f\"APIエラー: {e}\"}\n",
        "        # Claudeもエラー時にここにフォールバックする\n",
        "        return {\"profile\": f\"（API 1/3 エラー: {e}）\", \"query\": fallback_query, \"reason\": f\"APIエラー: {e}\"}\n",
        "\n",
        "\n",
        "# --- 【API 2/3】ナラティブ構築 (旧: RAG具体的事実の抽出) ---\n",
        "def summarize_rag_context(rag_query, mmr_results,\n",
        "                          api_key, google_api_key, claude_api_key,\n",
        "                          model,\n",
        "                          profile_text, # ◀ 追加: エピソード生成用\n",
        "                          theme):       # ◀ 追加: エピソード生成用\n",
        "\n",
        "    print(f\"    ... 【API 2/3】ナラティブ構築（体験＋知識）を実行中 (Model: {model})...\")\n",
        "\n",
        "    # --- 1. スニペットテキストの構築 ---\n",
        "    snippets_text = \"\"\n",
        "    has_snippets = False\n",
        "\n",
        "    if mmr_results and len(mmr_results) > 0:\n",
        "        has_snippets = True\n",
        "        for i, res in enumerate(mmr_results):\n",
        "            snippets_text += f\"--- スニペット {i+1} (出典: {res.get('url', 'N/A')}) ---\\n\"\n",
        "            snippets_text += f\"{res.get('snippet', '')}\\n\\n\"\n",
        "\n",
        "    # --- 2. プロンプトの分岐 (事実あり vs 事実なし) ---\n",
        "    if has_snippets:\n",
        "        # パターンA：検索結果あり（事実と体験を融合）\n",
        "        user_prompt = f\"\"\"\n",
        "あなたは優秀なノンフィクション作家です。\n",
        "提供された「検索結果」から客観的事実を抽出し、それを「ペルソナ」の生活に落とし込んだ「体験」として構成してください。\n",
        "\n",
        "# 1. シミュレーション対象（ペルソナ）\n",
        "{profile_text}\n",
        "\n",
        "# 2. 検索クエリ\n",
        "{rag_query}\n",
        "\n",
        "# 3. 参考スニペット\n",
        "{snippets_text}\n",
        "\n",
        "# 指示\n",
        "以下の2つのセクションを必ず出力してください。\n",
        "\n",
        "【知識】\n",
        "スニペットから、テーマに関連する「客観的な事実・数値・議論」を簡潔に抽出して箇条書きにしてください。\n",
        "\n",
        "【体験】\n",
        "知識も考慮し、このペルソナが日常生活の中で直面した、テーマに関連した印象的な出来事を「心の揺れ」や「感覚(五感)」を踏まえて、150文字程度で詳細に描写してください。\n",
        "\n",
        "\"\"\"\n",
        "    else:\n",
        "        # パターンB：検索結果なし（ペルソナからの想像のみ）\n",
        "        print(\"    ... 要約するスニペットがありませんでした。想像モードでエピソードを作成します。\")\n",
        "        user_prompt = f\"\"\"\n",
        "あなたは優秀な小説家です。\n",
        "今回の検索では具体的な情報が得られませんでしたが、この「ペルソナ」の性格と生活環境に基づき、テーマ「{theme}」に関して彼/彼女が経験したであろう日常の一コマをシミュレートしてください。\n",
        "\n",
        "# 1. シミュレーション対象（ペルソナ）\n",
        "{profile_text}\n",
        "\n",
        "# 2. テーマ\n",
        "{theme}\n",
        "\n",
        "# 指示\n",
        "以下の2つのセクションを必ず出力してください。\n",
        "\n",
        "\n",
        "【体験】\n",
        "このペルソナがテーマに関して最近経験したであろう「印象的な出来事」や「心の揺れ」を、五感（音、光、匂いなど）を用いたリアリティのある文章で150文字程度で描写してください。\n",
        "（事実データがないため、あくまで主観的な体験として書いてください）\n",
        "\n",
        "【知識】\n",
        "（検索結果がないため、以下のように記述してください）\n",
        "・特になし（自身の生活実感のみに基づく）\n",
        "\"\"\"\n",
        "    try:\n",
        "        summary = \"\"\n",
        "\n",
        "        # ▼▼▼ モデル分岐 ▼▼▼\n",
        "        if model.startswith(\"gemini-\"):\n",
        "            client = get_gemini_client(google_api_key, model)\n",
        "            generation_config = genai.types.GenerationConfig(temperature=0.0)\n",
        "            response = client.generate_content(user_prompt, generation_config=generation_config)\n",
        "            summary = response.text\n",
        "\n",
        "        elif model.startswith(\"gpt-\"):\n",
        "            client = get_openai_client(api_key)\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            summary = response.choices[0].message.content.strip()\n",
        "\n",
        "        elif model.startswith(\"claude-\"): # ◀◀◀ Claude分岐を追加\n",
        "            client = get_claude_client(claude_api_key)\n",
        "            response = client.messages.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=500\n",
        "            )\n",
        "            summary = response.content[0].text\n",
        "\n",
        "        else:\n",
        "            raise Exception(f\"未対応のモデルです: {model}\")\n",
        "        # ▲▲▲ モデル分岐 ▲▲▲\n",
        "\n",
        "        print(f\"    ... 事実抽出完了。\")\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ... 事実抽出APIエラー: {e}\")\n",
        "        if \"BLOCK_REASON_SAFETY\" in str(e):\n",
        "             print(\"    ... ★★★ Geminiのセーフティ機能によりブロックされました ★★★\")\n",
        "             return f\"（事実抽出APIエラー: Geminiによりブロック）\"\n",
        "        return f\"（事実抽出APIエラー: {e}）\"\n",
        "\n",
        "\n",
        "\n",
        "# --- 【API 3/3】意見生成 (V7: 高スコア版シンプルプロンプト) ---\n",
        "def generate_and_parse_all_opinions(\n",
        "    theme,\n",
        "    stance,\n",
        "    profile_text,\n",
        "    context_text,\n",
        "    volume_text,\n",
        "    malice_type,\n",
        "    api_key,\n",
        "    google_api_key,\n",
        "    claude_api_key, # ◀ 引数追加\n",
        "    model,\n",
        "    chunk_count,\n",
        "    logical_part,\n",
        "    emotional_part):\n",
        "\n",
        "    print(f\"    ... 【API 3/3】「5つの意見と確率」をリクエスト中 (Model: {model})...\")\n",
        "\n",
        "    # (malice_instruction と user_prompt は変更なし)\n",
        "    context_section = \"\"\n",
        "    if context_text:\n",
        "        context_section = f\"■ 参照知識・具体的体験：\\n{context_text}\\n\"\n",
        "    malice_instruction = \"\"\n",
        "    if malice_type in [\"早まった一般化\", \"対人攻撃\", \"藁人形論法\"]:\n",
        "        malice_instruction = f\"（追加指示：この意見には、意図的に「{malice_type}」という論理的誤謬を含めてください）\"\n",
        "        context_section = f\"■ 参照知識・体験：\\n{context_text}\\n\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "以下の設定に基づき、この人物が抱くであろう意見の確率分布をシミュレートしてください。\n",
        "\n",
        "■ シミュレーション対象（ペルソナ）：\n",
        "{profile_text}\n",
        "\n",
        "{context_section}\n",
        "\n",
        "■ タスク：\n",
        "テーマ「{theme}」に対し、この人物が**【{stance}】**という立場を堅持したまま、体験や知識に基づいた5つのリアリティある意見（各{volume_text}）を出力せよ。\n",
        "※この人物の現在の思考・口調バランスは【論理{logical_part}割 ： 感情 {emotional_part}割】です。\n",
        "　(感情優位ならオノマトペを交えて文体などを崩し、論理優位なら数字などを用いて淡々と記述してください。)\n",
        "{malice_instruction}\n",
        "\n",
        "■ 確率分布の条件：\n",
        "「{stance}」の枠内で、統計的に最もありふれた意見（0.40+）から、最も出現しにくい主観的な意見（0.05）まで、グラデーションをつけてシミュレートすること。\n",
        "\n",
        "■ シミュレーション結果の出力形式：\n",
        "1. （ここに1つ目の意見） (0.40)\n",
        "2. （ここに2つ目の意見） (0.30)\n",
        "3. （ここに3つ目の意見） (0.15)\n",
        "4. （ここに4つ目の意見） (0.10)\n",
        "5. （ここに5つ目の意見） (0.05)\n",
        "\n",
        "今、この人物ならどのような言葉でこのテーマを語るだろうか？\n",
        "\"\"\"\n",
        "    full_prompt = user_prompt\n",
        "    full_response_text = \"\"\n",
        "\n",
        "    try:\n",
        "        # ▼▼▼ モデル分岐 ▼▼▼\n",
        "        if model.startswith(\"gemini-\"):\n",
        "            client = get_gemini_client(google_api_key, model)\n",
        "            generation_config = genai.types.GenerationConfig(temperature=1.0, top_p=1.0)\n",
        "            response = client.generate_content(user_prompt, generation_config=generation_config)\n",
        "            full_response_text = response.text\n",
        "\n",
        "        elif model.startswith(\"gpt-\"):\n",
        "            client = get_openai_client(api_key)\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=1.0,\n",
        "                top_p=1.0,\n",
        "                max_tokens=1500\n",
        "            )\n",
        "            full_response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        elif model.startswith(\"claude-\"): # ◀◀◀ Claude分岐を追加\n",
        "            client = get_claude_client(claude_api_key)\n",
        "            response = client.messages.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=1.0,\n",
        "                max_tokens=1500\n",
        "            )\n",
        "            full_response_text = response.content[0].text\n",
        "\n",
        "        else:\n",
        "            raise Exception(f\"未対応のモデルです: {model}\")\n",
        "        # ▲▲▲ モデル分岐 ▲▲▲\n",
        "\n",
        "        # (確率的発想の「解析ロジック」は変更なし)\n",
        "        prob_pattern = re.compile(\n",
        "            r\"[\\(（]\\s*(?:(?:確率|probability)\\s*[:：]?)?\\s*(\\d+\\.?\\d*)\\s*[%]?\\s*[\\)）]\\s*$\",\n",
        "            re.IGNORECASE\n",
        "        )\n",
        "        opinion_blocks = re.split(r'^\\s*(?:\\d+|[①②③④⑤])[.)―]?\\s+', full_response_text, flags=re.MULTILINE)\n",
        "        candidates = []\n",
        "        for block in opinion_blocks:\n",
        "            if not block.strip():\n",
        "                continue\n",
        "            match = prob_pattern.search(block)\n",
        "            if match:\n",
        "                try:\n",
        "                    prob_str = match.group(1)\n",
        "                    prob_val = float(prob_str)\n",
        "                    if prob_val > 1.0:\n",
        "                        prob_val = prob_val / 100.0\n",
        "                    opinion_text = prob_pattern.sub('', block).strip()\n",
        "                    if opinion_text:\n",
        "                        candidates.append((opinion_text, prob_val))\n",
        "                except Exception as e:\n",
        "                    print(f\"    ... 確率の解析中に一部エラー: {e}\")\n",
        "            else:\n",
        "                print(f\"    ... 警告: 確率を解析できないブロックがありました。無視します。 -> '{block[:50]}...'\")\n",
        "\n",
        "        parsed_count = len(candidates)\n",
        "        if parsed_count > 0:\n",
        "            print(f\"    ... {parsed_count}件の候補を解析。\")\n",
        "            return candidates, full_prompt, full_response_text\n",
        "        else:\n",
        "            print(f\"    ... 警告: 確率の解析に失敗。レスポンス全体を1つの意見として扱します。\")\n",
        "            fallback_opinion = re.sub(r'^\\s*(?:\\d+[.)]|\\*|\\-|\\・)\\s+', '', full_response_text).strip()\n",
        "            candidates = [(fallback_opinion, -1.0)]\n",
        "            return candidates, full_prompt, full_response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"APIの呼び出し中にエラーが発生しました: {e}\"\n",
        "        if \"BLOCK_REASON_SAFETY\" in str(e):\n",
        "             print(\"    ... ★★★ Geminiのセーフティ機能によりブロックされました ★★★\")\n",
        "             error_message = f\"API Error: Geminiによりブロックされました\"\n",
        "        return f\"API Error: {error_message}\", None, \"\"\n",
        "\n",
        "\n",
        "\n",
        "# --- ★★★ メインの処理関数 (V7: 最低確率1件を保存) ★★★\n",
        "def generate_opinions_only(theme, num_outputs, num_pro, num_neutral, num_con,\n",
        "                            malice_choice, num_hasty, num_ad_hominem, num_straw_man,\n",
        "                            unified_api_key,\n",
        "                            model):\n",
        "\n",
        "    # ◀◀◀【変更点 4/5】Google Driveをマウント（連携）する\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Driveのマウントに成功しました。\")\n",
        "    except Exception as e:\n",
        "        print(f\"Google Driveのマウントに失敗しました: {e}\")\n",
        "    # ▲▲▲ ここまで追加 ▲▲▲\n",
        "\n",
        "    # ▼▼▼ 【追加】キーの振り分けロジック ▼▼▼\n",
        "    # まず、すべてのキーを「空」に初期化\n",
        "    api_key = None\n",
        "    google_api_key = None\n",
        "    claude_api_key = None\n",
        "\n",
        "    # 選択されたモデル名に応じて、1つのキーを正しい変数に割り当てる\n",
        "    if model.startswith(\"gpt-\"):\n",
        "        api_key = unified_api_key\n",
        "        print(\"OpenAI APIキー（UI入力）を使用します。\")\n",
        "    elif model.startswith(\"gemini-\"):\n",
        "        google_api_key = unified_api_key\n",
        "        print(\"Google AI APIキー（UI入力）を使用します。\")\n",
        "    elif model.startswith(\"claude-\"):\n",
        "        claude_api_key = unified_api_key\n",
        "        print(\"Anthropic APIキー（UI入力）を使用します。\")\n",
        "    # ▲▲▲ 【追加】ここまで ▲▲▲\n",
        "\n",
        "    if not theme:\n",
        "        return \"「テーマ」を入力してください。\", None, None, None\n",
        "\n",
        "    # ▼▼▼ APIキーの警告をモデル別に変更 ▼▼▼\n",
        "    if model.startswith(\"gpt-\") and not api_key:\n",
        "        print(\"警告: OpenAI APIキーが入力されていません。Colab Secretsを試行します...\")\n",
        "    if model.startswith(\"gemini-\") and not google_api_key:\n",
        "        print(\"警告: Google APIキーが入力されていません。Colab Secretsを試行します...\")\n",
        "    if model.startswith(\"claude-\") and not claude_api_key: # ◀ 追加\n",
        "        print(\"警告: Anthropic APIキーが入力されていません。Colab Secretsを試行します...\")\n",
        "    # ▲▲▲ 変更ここまで ▲▲▲\n",
        "\n",
        "    if not mmr_model:\n",
        "        return \"❌ 実行エラー: MMRモデルのロードに失敗しました。ランタイムを再起動してください。\", None, None, None\n",
        "\n",
        "    # ... (スタンスと悪意のリストを作成する部分は変更なし) ...\n",
        "    num_pro = int(num_pro) if num_pro is not None else 0\n",
        "    num_neutral = int(num_neutral) if num_neutral is not None else 0\n",
        "    num_con = int(num_con) if num_con is not None else 0\n",
        "    num_hasty = int(num_hasty) if num_hasty is not None else 0\n",
        "    num_ad_hominem = int(num_ad_hominem) if num_ad_hominem is not None else 0\n",
        "    num_straw_man = int(num_straw_man) if num_straw_man is not None else 0\n",
        "    num_opinions_to_generate = int(num_outputs)\n",
        "    stances = []\n",
        "    if (num_pro + num_neutral + num_con) == num_opinions_to_generate:\n",
        "        stances.extend(['賛成'] * num_pro)\n",
        "        stances.extend(['中立'] * num_neutral)\n",
        "        stances.extend(['反対'] * num_con)\n",
        "    else:\n",
        "        print(f\"立場の内訳が目標意見数（{num_opinions_to_generate}）と一致しないため、ランダムに割り当てます。\")\n",
        "        stances = random.choices(['賛成', '中立', '反対'], k=num_opinions_to_generate)\n",
        "    malice_types = []\n",
        "    if malice_choice == \"悪意あり\":\n",
        "        malice_types.extend([\"早まった一般化\"] * num_hasty)\n",
        "        malice_types.extend([\"対人攻撃\"] * num_ad_hominem)\n",
        "        malice_types.extend([\"藁人形論法\"] * num_straw_man)\n",
        "        num_specified_malice = len(malice_types)\n",
        "        num_remaining = num_opinions_to_generate - num_specified_malice\n",
        "        if num_remaining > 0:\n",
        "            malice_types.extend([\"なし\"] * num_remaining)\n",
        "        else:\n",
        "            malice_types = malice_types[:num_opinions_to_generate]\n",
        "    else:\n",
        "        malice_types.extend([\"なし\"] * num_opinions_to_generate)\n",
        "    random.shuffle(stances)\n",
        "    random.shuffle(malice_types)\n",
        "    combined_attributes = list(zip(stances, malice_types))\n",
        "    all_rows_data = []\n",
        "    formatted_output = \"\"\n",
        "    total_opinions_generated = 0\n",
        "    print(f\"合計 {num_opinions_to_generate} 件の意見生成（1件あたりAPI 3回＋RAG）を開始します...\")\n",
        "\n",
        "    # --- メインループ (V7) ---\n",
        "    for i, (base_stance, malice_type) in enumerate(combined_attributes):\n",
        "        opinion_id = i + 1\n",
        "\n",
        "        # ◀◀◀ ここで思考比率を決定 1/3 ▶▶▶\n",
        "        RATIO_OPTIONS = [\"10:0\", \"9:1\", \"8:2\", \"7:3\", \"6:4\", \"5:5\", \"4:6\", \"3:7\", \"2:8\", \"1:9\", \"0:10\"]\n",
        "        WEIGHTS = [2, 3, 5, 9, 13, 15, 13, 9, 5, 3, 2]\n",
        "        chosen_ratio = random.choices(RATIO_OPTIONS, weights=WEIGHTS, k=1)[0]\n",
        "        logical_part, emotional_part = chosen_ratio.split(':')\n",
        "\n",
        "        # ★ 実験2：API 3に渡すための「詳細な立場」をここで決定\n",
        "        if base_stance == \"賛成\":\n",
        "            detailed_stance = random.choice([\n",
        "                # --- 強い推進（分布の端を形成） ---\n",
        "                \"全面的に賛成\", \"確信を持って賛成\", \"強く賛成\",\n",
        "                # --- 標準的な賛成（分布の中核） ---\n",
        "                \"概ね賛成\", \"どちらかといえば賛成\", \"葛藤しつつも賛成\",\n",
        "                # --- 境界線上の賛成（反対派と混ざる領域） ---\n",
        "                \"一応賛成\", \"しぶしぶ賛成\", \"やむを得ず賛成\", \"条件付きで賛成\"\n",
        "            ])\n",
        "        elif base_stance == \"反対\":\n",
        "            detailed_stance = random.choice([\n",
        "                # --- 強い拒絶（反対側の端） ---\n",
        "                \"全面的に反対\", \"断固として反対\", \"強く反対\",\n",
        "                # --- 標準的な反対 ---\n",
        "                \"概ね反対\", \"どちらかといえば反対\", \"葛藤しつつも反対\",\n",
        "                # --- 境界線上の反対（賛成派と混ざる領域） ---\n",
        "                \"一応反対\", \"しぶしぶ反対\", \"やむを得ず反対\", \"条件付きで反対\"\n",
        "            ])\n",
        "        else:\n",
        "          detailed_stance = \"中立\"\n",
        "\n",
        "        #print(f\"--- 意見 {opinion_id}/{num_opinions_to_generate} (立場: {stances}) を生成中 ---\")\n",
        "        print(f\"--- 意見 {opinion_id}/{num_opinions_to_generate} (立場: {base_stance}  を生成中 ---\")\n",
        "        random_persona = {key: random.choice(values) for key, values in PERSONA_ATTRIBUTES.items()}\n",
        "        print(f\"  ... 基本属性: {random_persona['age']}, {random_persona['personality']}, {random_persona['volume']}\")\n",
        "\n",
        "        # ★ ステップ1: プロフィールとクエリを同時生成 (API 1/3)\n",
        "        api1_response = generate_profile_and_query(\n",
        "            random_persona,\n",
        "            theme,\n",
        "            base_stance,\n",
        "            api_key,\n",
        "            google_api_key,\n",
        "            claude_api_key, # ◀ 引数追加\n",
        "            model=model\n",
        "        )\n",
        "        profile_text = api1_response[\"profile\"]\n",
        "        rag_query = api1_response[\"query\"]\n",
        "        query_reason = api1_response[\"reason\"]\n",
        "\n",
        "        # ★ ステップ2.5: ハイブリッドRAG\n",
        "        ddgs_search_results = perform_web_search(rag_query, max_results=20)\n",
        "        high_quality_chunks = scrape_and_chunk_pages(\n",
        "            ddgs_search_results,\n",
        "            top_n=SCRAPE_TOP_N,\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "        current_chunk_count = len(high_quality_chunks)\n",
        "        snippets_for_mmr = []\n",
        "        rag_source_type = \"\"\n",
        "        if high_quality_chunks:\n",
        "            print(f\"  ... [RAG] 高品質チャンク ({len(high_quality_chunks)}件) をMMRに渡します。\")\n",
        "            snippets_for_mmr = high_quality_chunks\n",
        "            rag_source_type = \"Scraped Chunks (Readability V6)\"\n",
        "        else:\n",
        "            print(f\"  ... [RAG] スクレイピング失敗。フォールバック (ddgs要約 {len(ddgs_search_results)}件) をMMRに渡します。\")\n",
        "            snippets_for_mmr = ddgs_search_results\n",
        "            rag_source_type = \"Fallback DDGS\"\n",
        "        mmr_results = apply_mmr(rag_query, snippets_for_mmr, top_k=5, lambda_val=0.5)\n",
        "\n",
        "        # ★ ステップ2.6: RAG 具体的事実抽出 (API 2/3)\n",
        "        clean_extracted_facts = summarize_rag_context(\n",
        "            rag_query,\n",
        "            mmr_results,\n",
        "            api_key,\n",
        "            google_api_key,\n",
        "            claude_api_key, # ◀ 引数追加\n",
        "            model=model,\n",
        "            profile_text=profile_text, # ★ここを追加\n",
        "            theme=theme\n",
        "\n",
        "        )\n",
        "        noisy_context_for_csv = \"\"\n",
        "        if mmr_results:\n",
        "            noisy_context_for_csv += f\"以下は、{rag_source_type} から抽出したスニペットです。\\n\"\n",
        "            for j, res in enumerate(mmr_results):\n",
        "                noisy_context_for_csv += f\"--- 情報 {j+1} (出典: {res.get('url', 'N/A')}) ---\\n{res.get('snippet', '')}\\n\"\n",
        "        else:\n",
        "            noisy_context_for_csv = \"（MMRによるスニペット選定結果なし）\"\n",
        "\n",
        "\n",
        "        # ★ ステップ3: 確率的意見生成 (API 3/3)\n",
        "        candidates_list, used_prompt, full_response = generate_and_parse_all_opinions(\n",
        "            theme=theme,\n",
        "            stance=detailed_stance,\n",
        "            profile_text=profile_text,\n",
        "            context_text=clean_extracted_facts,\n",
        "            volume_text=random_persona['volume'],\n",
        "            malice_type=malice_type,\n",
        "            api_key=api_key,\n",
        "            google_api_key=google_api_key,\n",
        "            claude_api_key=claude_api_key, # ◀ 引数追加\n",
        "            model=model,\n",
        "            chunk_count=current_chunk_count,\n",
        "            # ◀◀◀ 引数に追加 2/3 ▶▶▶\n",
        "            logical_part=logical_part,\n",
        "            emotional_part=emotional_part\n",
        "        )\n",
        "\n",
        "        # 4. APIエラーハンドリング\n",
        "        if not isinstance(candidates_list, list):\n",
        "            print(f\"    ... 意見 {opinion_id} でエラー発生。スキップします。エラー: {candidates_list}\")\n",
        "            formatted_output += f\"--- 意見 {opinion_id} (APIエラー) ---\\n{candidates_list}\\n\\n\"\n",
        "            continue\n",
        "\n",
        "        # 5. ★★★ V7: 最低確率の候補を1件だけ選定 ★★★\n",
        "        valid_candidates = [c for c in candidates_list if c[1] >= 0]\n",
        "        if not valid_candidates:\n",
        "            print(f\"    ... 意見 {opinion_id} で確率の解析に失敗。スキップします。\")\n",
        "            formatted_output += f\"--- 意見 {opinion_id} (全候補の確率解析エラー) ---\\n\\n\"\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # 確率が高い順（0.45 -> 0.05）に並んでいることを前提とします\n",
        "            # インデックス 0, 1, 2, 3 が「標準的な意見」、4 が「最低確率(0.05)」\n",
        "\n",
        "            if random.random() < 0.70:\n",
        "                # 70%の確率で「インデックス 4 (0.05の意見)」を選択\n",
        "                # 万が一、AIが4つしか出さなかった場合を考慮して min でガード\n",
        "                idx = min(4, len(valid_candidates) - 1)\n",
        "                selected_candidate = valid_candidates[idx]\n",
        "                selection_type = \"Lowest (70% route)\"\n",
        "            else:\n",
        "                # 30%の確率で「インデックス 0〜3」からランダムに選択\n",
        "                # 候補が少ない場合は全部から選ぶ\n",
        "                others = valid_candidates[:4] if len(valid_candidates) > 1 else valid_candidates\n",
        "                selected_candidate = random.choice(others)\n",
        "                selection_type = \"Random Others (30% route)\"\n",
        "\n",
        "            selected_opinion_text = selected_candidate[0]\n",
        "            selected_probability = selected_candidate[1]\n",
        "\n",
        "            print(f\"    ... {len(valid_candidates)}件中 '{selection_type}' で 確率:{selected_probability:.4f} を選択。\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ... 意見 {opinion_id} での意見選定中にエラー: {e}。スキップします。\")\n",
        "            formatted_output += f\"--- 意見 {opinion_id} (意見選定エラー: {e}) ---\\n\\n\"\n",
        "            continue\n",
        "\n",
        "        # 6. 選ばれた1件のみをCSVデータに追加\n",
        "        total_opinions_generated += 1\n",
        "        opinion_text_cleaned = re.sub(r'^\\s*(?:\\d+[.)]|\\*|\\-|\\・)\\s+', '', selected_opinion_text).strip()\n",
        "        row_data = {\n",
        "            \"Opinion_ID\": opinion_id,\n",
        "            \"生成された意見\": opinion_text_cleaned,\n",
        "            \"テーマ\": theme,\n",
        "            \"立場\": base_stance,\n",
        "            \"詳細な立場\": detailed_stance,\n",
        "            \"思考バランス(論理:感情)\": chosen_ratio,\n",
        "            \"Selection_Type\": selection_type,\n",
        "            \"悪意の有無\": \"なし\" if malice_type == \"なし\" else \"あり\",\n",
        "            \"悪意の種類\": malice_type if malice_type != \"あり\" else \"指定なし\",\n",
        "            \"性別\": random_persona['gender'],\n",
        "            \"年齢\": random_persona['age'],\n",
        "            \"性格\": random_persona['personality'],\n",
        "            \"口調\": random_persona['tone'],\n",
        "            \"文章量\": random_persona['volume'],\n",
        "            \"Profile\": profile_text,\n",
        "            \"RAG_Source_Type\": rag_source_type,\n",
        "            \"RAG_Query\": rag_query,\n",
        "            \"RAG_Query_Reason\": query_reason,\n",
        "            \"RAG_Clean_Context\": clean_extracted_facts,\n",
        "            \"RAG_Raw_Snippets\": noisy_context_for_csv,\n",
        "            \"Selected_Probability\": selected_probability,\n",
        "            \"Num_Candidates_Parsed\": len(candidates_list),\n",
        "            \"Num_Candidates_Valid\": len(valid_candidates),\n",
        "            \"生成に使用したプロンプト\": used_prompt,\n",
        "            \"Full_API_Response\": full_response\n",
        "        }\n",
        "        all_rows_data.append(row_data)\n",
        "\n",
        "        # 7. プレビューに選ばれた1件のみを表示\n",
        "        persona_str = f\"属性:{random_persona['age']},{random_persona['personality']} {profile_text}\"\n",
        "        prob_info = f\"(選択された確率:{selected_probability:.4f})\"\n",
        "        rag_info = f\"[{rag_source_type}]\"\n",
        "        formatted_output += f\"--- D {total_opinions_generated} (ID:{opinion_id}) {prob_info} {rag_info} ---\\n[ペルソナ: {persona_str}]\\n[RAGクエリ: {rag_query}]\\n[RAG抽出文: {clean_extracted_facts[:100]}...]\\n\\n{opinion_text_cleaned}\\n\\n\"\n",
        "\n",
        "        # ◀◀◀【変更点 5/5】レート制限対策のスリープを追加\n",
        "        if model.startswith(\"gemini-\"):\n",
        "            print(f\"  ... 意見 {opinion_id} 完了。[Gemini] レート制限対策のため 2秒待機します...\")\n",
        "            time.sleep(2) # ◀◀◀ 1件ごとに2秒待機\n",
        "        else:\n",
        "            print(f\"  ... 意見 {opinion_id} 完了。\")\n",
        "        # ▲▲▲ ここまで追加 ▲▲▲\n",
        "\n",
        "    print(f\"全 {num_opinions_to_generate} 件の生成が完了しました。\")\n",
        "\n",
        "    if not all_rows_data:\n",
        "        print(\"データが1件も生成されませんでした。\")\n",
        "        return \"❌ データが1件も生成されませんでした。APIキーやプロンプトを確認してください。\", \"\", None, None\n",
        "    try:\n",
        "        df = pd.DataFrame(all_rows_data)\n",
        "        filepath, status = save_to_csv(df)\n",
        "        print(status)\n",
        "        return status, formatted_output.strip(), df, filepath\n",
        "    except Exception as e:\n",
        "        print(f\"DataFrame作成またはCSV保存エラー: {e}\")\n",
        "        return f\"❌ DataFrameの作成またはCSVへの保存中にエラー: {e}\", \"\", None, None\n",
        "\n",
        "\n",
        "# --- Gradioインターフェースの定義 (説明文をV7に戻す) ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h1>人工意見データセットの自動構築アプリ</h1>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"<h5>▼ APIキー設定</h5>\")\n",
        "\n",
        "            # ▼▼▼ 3つの入力をこれ1つに置き換える ▼▼▼\n",
        "            unified_api_key_input = gr.Textbox(\n",
        "                label=\"APIキー\",\n",
        "                placeholder=\"sk-...\",\n",
        "                type=\"password\",\n",
        "                info=\"モデルに応じて、使用するAPIキー（OpenAI, Google, Anthropic）を1つ入力してください。\"\n",
        "            )\n",
        "            # ▲▲▲ 置き換えここまで ▲▲▲\n",
        "\n",
        "\n",
        "            # ▼▼▼ モデル選択の choices を修正 ▼▼▼\n",
        "            model_input = gr.Dropdown(\n",
        "                label=\"使用するモデル\",\n",
        "                choices=[\n",
        "                    \"gpt-4o\",\n",
        "                    \"gpt-3.5-turbo\",\n",
        "                    \"gpt-4-turbo\",\n",
        "                    \"gemini-2.5-flash\",\n",
        "                    \"gemini-2.5-pro\",\n",
        "                    \"claude-haiku-4-5\",\n",
        "                    \"claude-sonnet-4-5\",\n",
        "\n",
        "                ],\n",
        "                value=\"gpt-4o\",\n",
        "                info=\"API呼び出しに使用するモデルを選択します。\"\n",
        "            )\n",
        "            # ▲▲▲ 修正ここまで ▲▲▲\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            theme_input = gr.Textbox(label=\"テーマ\")\n",
        "            num_outputs_input = gr.Number(label=\"最終的な意見の数 (× 3 APIコール + RAG)\", value=3, step=1, minimum=1)\n",
        "\n",
        "            gr.Markdown(\"<h5>▼ 立場の内訳（合計が「最終的な意見の数」と合わない場合はランダム）</h5>\")\n",
        "            with gr.Row():\n",
        "                num_pro_input = gr.Number(label=\"賛成\", value=1, step=1, minimum=0)\n",
        "                num_con_input = gr.Number(label=\"反対\", value=1, step=1, minimum=0)\n",
        "\n",
        "            gr.Markdown(\"<h5>▼ 悪意の有無</h5>\")\n",
        "            malice_choice_radio = gr.Radio([\"悪意なし\", \"悪意あり\"], label=\"生成する意見に悪意を含めますか？\", value=\"悪意なし\")\n",
        "            with gr.Group(visible=False) as malice_details_group:\n",
        "                gr.Markdown(\"<h6>▼ 悪意の種類の内訳（指定がない分は「悪意なし」になります）</h6>\")\n",
        "                with gr.Row():\n",
        "                    num_hasty_input = gr.Number(label=\"早まった一般化\", value=0, step=1, minimum=0)\n",
        "                    num_ad_hominem_input = gr.Number(label=\"対人攻撃\", value=0, step=1, minimum=0)\n",
        "                    num_straw_man_input = gr.Number(label=\"藁人形論法\", value=0, step=1, minimum=0)\n",
        "            def toggle_malice_details(choice):\n",
        "                return gr.update(visible=choice == \"悪意あり\")\n",
        "            malice_choice_radio.change(fn=toggle_malice_details, inputs= malice_choice_radio, outputs=malice_details_group)\n",
        "\n",
        "            generate_btn = gr.Button(\"① 統合プロセス(V7)を実行し、CSVを出力\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            export_status_output = gr.Markdown(label=\"処理ステータス\")\n",
        "            opinions_output = gr.Textbox(label=\"生成された意見（プレビュー）\", lines=15, interactive=False)\n",
        "            dataframe_preview = gr.Dataframe(label=\"生成結果プレビュー（CSV内容）\", wrap=True, row_count=3)\n",
        "            csv_output_file = gr.File(label=\"ダウンロード\", interactive=False)\n",
        "\n",
        "    gr.Examples(\n",
        "        [[\"AIと人間の共存\", 3, 1, 1, 1, \"悪意なし\", 0, 0, 0]],\n",
        "        inputs=[theme_input, num_outputs_input, num_pro_input, num_neutral_input, num_con_input,\n",
        "                malice_choice_radio, num_hasty_input, num_ad_hominem_input, num_straw_man_input]\n",
        "    )\n",
        "\n",
        "    # ▼▼▼ generate_btn.click の inputs を修正 ▼▼▼\n",
        "    generate_btn.click(\n",
        "        fn=generate_opinions_only,\n",
        "        inputs=[\n",
        "            theme_input, num_outputs_input, num_pro_input, num_neutral_input, num_con_input,\n",
        "            malice_choice_radio, num_hasty_input, num_ad_hominem_input, num_straw_man_input,\n",
        "            unified_api_key_input,\n",
        "            model_input\n",
        "        ],\n",
        "        outputs=[export_status_output, opinions_output, dataframe_preview, csv_output_file]\n",
        "    )\n",
        "    # ▲▲▲ 修正ここまで ▲▲▲\n",
        "\n",
        "# Gradioの起動\n",
        "print(\"Gradioインターフェース(V7 - 1件選定)を起動します...\")\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "240Tw1ccE6gl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gGrp3zRdnmL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCU3G6762yEp78EQMZe+cX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}